{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88c9684-ce99-47f7-b6f1-1f13eae99e95",
   "metadata": {},
   "source": [
    "<a id=\"section-ChatBot\"></a>\n",
    "# Exemples d’IA non transparentes :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248a8a8-e02a-49b8-ad3c-d589051f1883",
   "metadata": {},
   "source": [
    "## TAY (2016) – Microsoft\n",
    "\n",
    "Tay était un chatbot créé par Microsoft pour interagir sur Twitter. Conçu pour apprendre automatiquement via les interactions humaines, il a rapidement adopté un langage raciste, sexiste et haineux, influencé par les utilisateurs eux-mêmes.\n",
    "\n",
    "- Échec de transparence : pas de contrôle sur l’apprentissage, pas de supervision humaine (HITL)  \n",
    "- Leçon : nécessité de filtrage, monitoring post-déploiement et mécanismes de contrôle éthique  \n",
    "(Source : Microsoft Research Blog, 2016)\n",
    "\n",
    "## Xiaolce – Chine\n",
    "\n",
    "Xiaolce est un chatbot développé pour entretenir des conversations affectives avec les utilisateurs, simulant une relation romantique. S’il montre des capacités impressionnantes d’interaction, il soulève aussi des questions sur la manipulation émotionnelle et la protection des données sensibles.\n",
    "\n",
    "- Problème potentiel : dépendance affective  \n",
    "- Enjeu : explicabilité des réponses, transparence des intentions du système  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ad769-1825-446a-84db-f41e4cd49480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
