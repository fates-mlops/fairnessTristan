{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7486835-2e7d-4a5b-a046-ac0826cff952",
   "metadata": {},
   "source": [
    "<a id=\"mots-cles\"></a>\n",
    "# <span style=\"color:#1E90FF;\"> Detailed definitions of acronyms related to AI transparency </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d3cef-b822-4691-b5a5-718c679ea704",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"general_definition\"></a><span style=\"color:#1E90FF;\">General definitions (AI, transparency, responsibility…)</span>\n",
    "\n",
    "- **<a id=\"ai\"></a>Artificial Intelligence (AI)**: Set of techniques enabling machines to simulate human cognitive functions (reasoning, learning, perception…).\n",
    "- **<a id=\"interoperability\"></a>Interoperability**: Ability of different systems to work together, share information, or standards. ([Source NCBI](https://www.ncbi.nlm.nih.gov/books/NBK541904/))\n",
    "- **<a id=\"transparency-paradox\"></a>Transparency Paradox**: Idea that too much transparency can harm understanding, security, or trust. ([Source Springer Nature Link](https://link.springer.com/article/10.1007/s11023-020-09528-5))\n",
    "- **<a id=\"algorithmic-opacity\"></a>Algorithmic Opacity**: A situation in which the decisions of an automated system are not directly understandable, or even incomprehensible. It is then necessary to delve deeper into the situation to understand the decisions of the automated system..\n",
    "- **<a id=\"counterproductive\"></a>Counterproductive**: Producing an effect opposite to the intended one (loss of legitimacy instead of trust).\n",
    "- **<a id=\"ML\"></a>ML (Machine Learning)**: Machine learning is a branch of artificial intelligence that allows a computer system to learn automatically from data, without being explicitly programmed for each specific task. The system improves its performance by recognizing patterns or models in the data and adapting to new information.\n",
    "![representation of machine learning in diagram](img/MachineLearning.png) [Source : k21academy](https://k21academy.com/microsoft-azure/ai-900/machine-learning-algorithms-use-cases/)\n",
    "- **<a id=\"MLOPS\"></a>MLOps**: *Machine Learning Operations* DevOps practices applied to deployment, maintenance, and monitoring of ML models. \n",
    "- **<a id=\"AI-Driven\"></a>AI-Driven**: An AI-driven system or process is one that uses artificial intelligence as its core engine to make decisions, automate tasks, or extract insights from data.\n",
    "- **<a id=\"AIA\"></a>AIA**: AIA can refer to Artificial Intelligence Application or the Artificial Intelligence Act, a European Union regulatory framework for AI systems\n",
    "- **<a id=\"BIAS\"></a>BIAS**: Bias in Artificial Intelligence refers to systematic errors or prejudices in data or algorithms that cause unfair, inaccurate, or discriminatory outcomes. Biases can arise from unrepresentative training data, flawed assumptions in model design, or human prejudices unintentionally embedded in the AI system. These biases can negatively impact decision-making processes and lead to ethical concerns, especially in sensitive domains like hiring, lending, or criminal justice. ([Source : Time](https://time.com/collections/the-ai-dictionary-from-allbusiness-com/7273920/definition-of-ai-bias/))\n",
    "\n",
    "### <a id=\"ASL\"></a><span style=\"color:#1E90FF;\">AI Safety Level (ASL)</span>\n",
    "\n",
    "The **AI Safety Level** corresponds to a classification of AI systems according to their level of safety, based on the potential risks and impacts they may cause.\n",
    "\n",
    "- **Level 1** : Low Safety \n",
    "  AI systems with limited risks, whose negative consequences are unlikely or minor. Generally used in contexts where an error has a low impact. \n",
    "- **Level 2** : Moderate Safety\n",
    "  Systèmes pouvant présenter des risques plus importants, nécessitant des mécanismes de contrôle et de surveillance pour limiter les effets indésirables potentiels.\n",
    "- **Level 3** : High Safety \n",
    "  Systèmes critiques où les défaillances peuvent entraîner des conséquences graves sur les personnes, les biens ou l’environnement. Une attention forte est portée à la robustesse, la fiabilité et la gestion des risques.\n",
    "- **Level 4** : Very High Safety\n",
    "  Systems with major risk potential, requiring strict guarantees, regular audits, and constant human supervision to ensure safety.\n",
    "\n",
    "This classification helps adapt safety requirements to different uses and guides the development and regulation of AI according to their criticality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a6452-737f-47b0-9128-d442b5063401",
   "metadata": {},
   "source": [
    "## <a id=\"trust_explainability\"></a><span style=\"color:#1E90FF;\">Trust and Explainability</span>\n",
    "\n",
    "- **<a id=\"intelligible-explanation\"></a>Intelligible Explanation**: Explanation given in simple language, understandable by a non-expert. In the current context, all technical terms must be popularized to be understood by the greatest number of people.\n",
    "- **<a id=\"local-explanations\"></a>Local Explanations**: Justifications regarding a specific decision made by a model (why the model chose to exclude a particular CV).\n",
    "- **<a id=\"global-explanations\"></a>Global Explanations**: Description of the general functioning of a model, useful to understand the underlying logic or to evaluate it overall.\n",
    "- **<a id=\"EIA\"></a>EIA**: An Ethical Impact Assessment (EIA) for Artificial Intelligence is a systematic process used to evaluate the potential ethical risks and consequences associated with the development, deployment, and use of AI systems. The goal of an EIA is to identify, anticipate, and mitigate ethical issues such as bias, privacy violations, transparency, accountability, and societal impact before an AI system is implemented. This process helps ensure that AI technologies align with ethical principles and respect human rights.  ([Source : UNESCO](https://unesdoc.unesco.org/ark:/48223/pf0000373434.locale=en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee286fa-162e-493f-92a9-0f62332961ab",
   "metadata": {},
   "source": [
    "## <a id=\"technical_concepts\"></a><span style=\"color:#1E90FF;\">Technical Concepts</span>\n",
    "\n",
    "- **<a id=\"logs\"></a>Logs (or history)**: Detailed recordings of actions or decisions of a system, useful for audits or traceability.\n",
    "- **<a id=\"audits\"></a>Audits**: Independent or official assessments aimed at verifying the compliance, ethics, or performance of a system. Logs or backup rules may be requested for certain audits, for example.\n",
    "- **<a id=\"post-deployment-monitoring\"></a>Post-deployment Monitoring**: Continuous monitoring of a system after its deployment to detect possible adverse effects. This monitoring is as important as the deployment of the system because depending on the problems caused by it, a huge number of people could be affected. ([Source : Stackmoxie](https://www.stackmoxie.com/blog/best-practices-for-monitoring-ai-systems/))\n",
    "- **<a id=\"recourse-mechanisms\"></a>Recourse Mechanisms**:Means allowing a user to contest or request the review of an automated decision, this concept is mandatory, particularly for post-deployment monitoring.\n",
    "- **<a id=\"homomorphic-encryption\"></a>Homomorphic Encryption**: Homomorphic encryption is a cryptographic technique that allows operations to be performed on encrypted data without the data having to be decrypted. ([Source : CNIL](https://www.cnil.fr/fr/definition/chiffrement-homomorphe#:~:text=Le%20chiffrement%20homomorphe%20est%20une,ci%20aient%20%C3%A0%20%C3%AAtre%20d%C3%A9chiffr%C3%A9es.))\n",
    "- **<a id=\"watermarking\"></a>Watermarking**: Watermarking refers to techniques used to embed hidden information into AI models or the data they generate, allowing the creator to claim ownership or verify its authenticity. This helps protect intellectual property, detect unauthorized use, and preserve the integrity of AI-generated content. Watermarks are designed to be imperceptible in normal use, but detectable by specific methods. ([Source : Datascientest](https://datascientest.com/en/ai-watermarking-all-you-need-to-know))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc21dc-ca59-48fe-abc3-25069bbc500b",
   "metadata": {},
   "source": [
    "## <a id=\"explainability_technologies\"></a><span style=\"color:#1E90FF;\">Explainability Technologies</span>\n",
    "\n",
    "- **<a id=\"SHAP\"></a>SHAP (Shapley Additive Explanations)**: SHAP or SHapley Addictive exPlanations is a technique that is used to assign a value to each feature, indicating its contribution to a model’s output. ([Source : Markovml](https://www.markovml.com/blog/lime-vs-shap))\n",
    "Example: Credit scoring is a good example as it utilizes SHAP to reveal the impact of variables like income and credit history on the final credit score\n",
    "- **<a id=\"LIME\"></a>LIME (Local Interpretable Model-agnostic Explanations)**: LIME, or Local Interpretable Model-agnostic Explanations, is a technique that generates local approximations to model predictions. ([Source : Markovml](https://www.markovml.com/blog/lime-vs-shap))\n",
    "Example: In the process of predicting sentiments with a neural network, LIME highlights important words in a specific prediction. \n",
    "- **<a id=\"WIT\"></a>WIT (What If Tool)**: Visual tool to test the effect of modifications on a model’s input data.\n",
    "- **<a id=\"HITL\"></a>HITL (Human In The Loop)**: Human In The Loop involves continuous human involvement throughout the AI's operational process. ([Source : DeepScribe](https://www.deepscribe.ai/resources/optimizing-human-ai-collaboration-a-guide-to-hitl-hotl-and-hic-systems#:~:text=HITL%20involves%20continuous%20human%20involvement,AI%20acting%20under%20strict%20supervision.))\n",
    "- **<a id=\"HOTL\"></a>HOTL (Human On The Loop)**: Human On The Loop describes systems where humans monitor AI actions and can intervene if needed but do not participate in every decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca259e-316f-4d93-89a9-e119bfa44c5f",
   "metadata": {},
   "source": [
    "## <a id=\"compliance_standards\"></a><span style=\"color:#1E90FF;\">Compliance and Standards</span>\n",
    "\n",
    "- **<a id=\"AI-Act\"></a>AI Act**: European regulation on AI classifying systems by risk level and imposing specific requirements. ([Source : CNIL](https://www.cnil.fr/en/entry-force-european-ai-regulation-first-questions-and-answers-cnil))\n",
    "- **<a id=\"ISO-standards\"></a>ISO Standards**: International technical standards, also applicable to AI governance and security.\n",
    "- **<a id=\"GDPR\"></a>GDPR (General Data Protection Regulation)**: European regulation protecting personal data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53236fa-e6d9-4ca4-bfae-99178d2b933f",
   "metadata": {},
   "source": [
    "## <a id=\"communication_pedagogy\"></a><span style=\"color:#1E90FF;\">Communication and Pedagogy</span>\n",
    "\n",
    "- **<a id=\"pedagogical-translation\"></a>Pedagogical Translation**: Adapting technical discourse to make it understandable for a non-specialist audience.\n",
    "- **<a id=\"adapted-language\"></a>Adapted Language**: Choosing words or examples accessible according to the target audience’s level.\n",
    "- **<a id=\"contextual-transparency\"></a>Contextual Transparency**: Adjust the level of transparency based on the role, the needs of the recipient and their level of confidentiality (knowing what information they have access to).\n",
    "- **<a id=\"end-users\"></a>End Users**: People directly affected by the use or decisions of an AI system (citizens, customers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d2432-50e5-4143-bf4e-b6fc9d09e46b",
   "metadata": {},
   "source": [
    "## <a id=\"institutions_acronyms\"></a><span style=\"color:#1E90FF;\">Institutions, Frameworks, and Specialized Acronyms</span>\n",
    "\n",
    "- **<a id=\"IRIT\"></a>IRIT**: Research Institute in Computer Science of Toulouse — a multidisciplinary research center involved in ethics and AI regulation.([See the website](https://www.univ-tlse3.fr/structures-de-recherche/institut-de-recherche-en-informatique-de-toulouse))\n",
    "- **<a id=\"FATES\"></a>FATES**: *Fairness, Accountability, Transparency, Ethics, Security* Framework structuring ethical and sociotechnical requirements in AI.\n",
    "- **<a id=\"HAL\"></a>HAL**: *Hyper Articles en Ligne* French open archive platform for scientific publications.  ([See the HAL site](https://hal.archives-ouvertes.fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8636269-27ed-4814-bfa7-a654150b87a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
