{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938814d7-6a97-4a98-b8da-b2d78e057518",
   "metadata": {},
   "source": [
    "# <span style=\"color:#1E90FF;\"> The concept of transparency on [AI](../Source/Keywords.ipynb#ai) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac285cd7-6907-478f-ac17-32318e05c563",
   "metadata": {},
   "source": [
    "# <span style=\"color:#1E90FF;\">Table of Contents</span>\n",
    "\n",
    "1. Introduction\n",
    "2. Understanding Transparency in AI\n",
    "   - Does Everyone Understand Transparency the Same Way?\n",
    "   - Between Transparency and Privacy\n",
    "3. Explainability: Understanding AI Decisions\n",
    "4. Risks\n",
    "   - Exposure of Industrial Secrets\n",
    "   - Personal Data Leakage\n",
    "   - Tension Between Transparency and Security\n",
    "   - Governance and Accountability: Knowing Who Is Responsible\n",
    "5. Clarity for the Public: Readability and Accessibility\n",
    "6. Regulatory Compliance: AI Act and International Standards\n",
    "7. Future Perspectives\n",
    "   - Toward “Useful” Transparency\n",
    "   - The Role of Data and AI Literacy\n",
    "   - The Importance of Co-Constructing Standards\n",
    "8. Conclusion\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f136e16-e436-4b00-9c0d-2a66292b46ac",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">INTRODUCTION</span>  \n",
    "Transparency in AI is currently a topic of debate regarding **ethics** and **regulation**. This transparency aims to strengthen <span style=\"color:#DC143C;\">**trust**</span>, <span style=\"color:#DC143C;\">**accountability**</span>, and <span style=\"color:#DC143C;\">**acceptability**</span> of artificial intelligence systems ([Springer source](https://link.springer.com/article/10.1007/s00146-020-00960-w)). However, implementing full transparency is challenging due to the <span style=\"color:#FFA500;\">**availability of information**</span>, its <span style=\"color:#FFA500;\">**readability**</span>, and the data related to <span style=\"color:#FF0000;\">**privacy**</span>. We will explore how to address these paradoxes by designing transparency that is both <span style=\"color:#228B22;\">**relevant**</span> and <span style=\"color:#228B22;\">**secure**</span>.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d388440-1f9b-48f3-a1a4-8c566d631cdb",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Understanding Transparency in AI</span>\n",
    "\n",
    "Transparency in artificial intelligence (AI) means making the system’s decisions visible, understandable, and explainable. This is a major issue today with the increasing number of decisions made automatically such as analyzing a resume or sorting data. These [AI-driven](../Source/Keywords.ipynb#AI-driven) decisions are directly influenced/guided by machine learning ([ML](../Source/Keywords.ipynb#ML)) models. In this context, transparency becomes essential to ensure **fairness**, **reliability**, and **accountability**.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Does everyone understand transparency in the same way?</span>\n",
    "\n",
    "Making an AI system “transparent” does not guarantee that its functioning will be **<span style=\"color:#228B22;\">truly understood</span>**. **<span style=\"color:#FF8C00;\">Data visualization</span>** or the **<span style=\"color:#FF8C00;\">display of technical parameters</span>** can actually lead to more **<span style=\"color:#FF4500;\">cognitive opacity</span>** if the information is not simplified for the target audience. This shifts the issue from [algorithmic opacity](../Source/Keywords.ipynb#algorithmic-opacity) to **<span style=\"color:#FF4500;\">interpretive misunderstanding</span>** the problem is no longer the system’s “black box,” but the complexity of the language used to describe it.\n",
    "\n",
    "This phenomenon is highlighted in a study conducted by Stanford ([Foundation Model Transparency Index, 2023](https://crfm.stanford.edu/fmti/October-2023/index.html)), which explores mechanisms of **<span style=\"color:#228B22;\">trust in algorithmic systems</span>**. The study shows that providing too many technical details about internal parameters or justifications can actually **<span style=\"color:#FF0000;\">reduce user trust</span>** instead of increasing it. This underlines the crucial importance of offering **<span style=\"color:#228B22;\">[intelligible explanations](../Source/keywords.ipynb#intelligible-explanation)</span>**, not for experts, but for end users who are often non specialists ([study on PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC7755865/)).\n",
    "\n",
    "In this vein, transparency is not about delivering a **quantity of information**; it’s rather a **pedagogical translation effort**.  \n",
    "This involves:  \n",
    "- Choosing an appropriate level of language,  \n",
    "- Explaining algorithmic choices using metaphors, illustrations, or concrete comparisons,  \n",
    "- Anticipating the needs for simplification of technical topics for various audiences (citizens, professionals, decision makers).\n",
    "\n",
    "This principle aligns with the idea of “targeted transparency” or “explanation by design” developed within the field of **XAI (eXplainable AI)**: an explainable AI is not only readable by the machine or the developer, but must be **useful, accessible, and interpretable by the human** who uses it. The challenge is thus not only to show, but to ensure the user **understands** what they are using and how it works in light of technological advancements.\n",
    "\n",
    "[Read the full article on PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC7268993/)\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Between Transparency and Confidentiality</span>\n",
    "\n",
    "The \"**[transparency paradox](../Source/Keywords.ipynb#transparency-paradox)**\" refers to the difficulty of striking a balance: too little information undermines <span style=\"color:#DC143C;\">**trust**</span>, but too much information causes <span style=\"color:#FF8C00;\">**confusion**</span>, leading to a loss of <span style=\"color:#DC143C;\">**legitimacy**</span>. Full transparency can therefore become <span style=\"color:#FF0000;\">**[counterproductive](../Source/Keywords.ipynb#Counterproductive)**</span>, even <span style=\"color:#FF0000;\">**dangerous**</span>. This is also evident in confidential matters such as users’ private information. To achieve good transparency, [logs](../Source/keywords.ipynb#logs) and explanations about data use would be ideal. However, when it comes to user privacy, the legitimacy of such information requests must be questioned.\n",
    "\n",
    "This observation highlights several key aspects of the **transparency paradox**:\n",
    "\n",
    "- **<span style=\"color:#DC143C;\">Insufficient transparency</span>** can undermine **<span style=\"color:#DC143C;\">user trust</span>** and fuel distrust in AI systems.  \n",
    "- Conversely, **<span style=\"color:#FF8C00;\">overly exhaustive transparency</span>** can lead to **<span style=\"color:#FF8C00;\">confusion</span>**, paradoxically weakening the **<span style=\"color:#DC143C;\">perceived legitimacy</span>** of automated systems.  \n",
    "- This phenomenon, known as the **<span style=\"color:#FF0000;\">transparency paradox</span>**, refers to the fact that an overload of poorly organized or overly technical information can make understanding more difficult potentially becoming **<span style=\"color:#FF0000;\">counterproductive</span>** on an operational level.\n",
    "\n",
    "[Read the full article on Springer](https://link.springer.com/article/10.1007/s00146-020-00960-w)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcdf6dc-9eb3-4e9d-bcf4-20c2a216e251",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Explainability: Understanding AI Decisions</span>\n",
    "\n",
    "A fundamental aspect of transparency is the explainability of decisions. It consists of two main components:\n",
    "\n",
    "- **[Local explanations](../Source/Keywords.ipynb#local-explanations)**, which help to understand why a specific decision was made. Tools like **[SHAP](../Source/keywords.ipynb#SHAP)** (Shapley Additive Explanations) and **[LIME](../Source/keywords.ipynb#LIME)** (Local Interpretable Model Agnostic Explanations) are now widely used. They help explain how a model arrived at an individual prediction for example, why a person was denied a loan.\n",
    "\n",
    "- **[Global explanations](../Source/Keywords.ipynb#global-explanations)**, which aim to clarify the overall functioning of a model. This is particularly useful for regulators or researchers who need to assess the broader risks of a system.\n",
    "\n",
    "Tools like **[WIT](../Source/keywords.ipynb#WIT)** (What If Tool) allow users to visually test the effects of input changes on the model's outputs, thereby enhancing transparency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571cb56-db03-4e6f-9600-8989938c1dc3",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">RISKS</span>  \n",
    "\n",
    "### <span style=\"color:#8B008B;\">Exposure of Trade Secrets</span>\n",
    "\n",
    "Disclosing the algorithms and internal mechanisms of AI systems, although essential for transparency, presents a significant risk to **<span style=\"color:#8B008B;\">intellectual property</span>** and business competitiveness. Take, for example, the architectural plans of a new shopping center if someone could skillfully query the AI and partially or even fully recreate the work of a private company, then proprietary design knowledge and employees’ labor would essentially be **stolen** by the person conducting the inquiry.\n",
    "\n",
    "A *Forbes* article highlights the need to be cautious of the “**transparency paradox**”: in trying to reveal everything, companies expose themselves to risks of **losing competitive advantage** and falling victim to **malicious extraction strategies** ([Forbes, 2021](https://www.forbes.com/sites/aparnadhinakaran/2021/09/10/overcoming-ais-transparency-paradox/)).\n",
    "\n",
    "However, this risk can be mitigated. Certain protective techniques such as **[homomorphic encryption](../Source/keywords.ipynb#homomorphic-encryption)**, **[watermarking](../Source/keywords.ipynb#watermarking)**, or **federated learning** now make it possible to preserve confidentiality while enhancing explainability. Research into balancing transparency and industrial protection is actively evolving.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Leakage of Personal Data</span>\n",
    "\n",
    "Explainability also introduces a serious risk to **<span style=\"color:#8B008B;\">data privacy</span>**.  \n",
    "So-called **membership inference attacks** can use explanations to determine whether a specific data point was part of the training set, thereby exposing private information. Likewise, **prompt leaks** in generative models can unintentionally reveal fragments of sensitive data embedded in the training corpus.  \n",
    "A study published on *arXiv* details these vulnerabilities, showing that personal information such as names, addresses, or medical histories can be compromised through the explanations generated ([arXiv, 2019](https://arxiv.org/abs/1907.00164)).\n",
    "\n",
    "It is important to recognize that these risks depend on the **explainability methods** used by communicators and the **protective protocols** in place. For instance, techniques like **differential privacy** can help mitigate such leaks.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Tension Between Transparency and Security</span>\n",
    "\n",
    "The **<span style=\"color:#FF4500;\">tension between transparency and security</span>** illustrates a well known dilemma often criticized, but with practical relevance.  \n",
    "Overly broad or poorly controlled disclosure of technical details can provide attackers with the information they need to exploit **system vulnerabilities**.  \n",
    "A publication on *arXiv* shows that complete transparency (without privacy filters) can increase the attack surface, inherently reducing the robustness of AI systems ([arXiv, 2025](https://arxiv.org/abs/2501.18669)).\n",
    "\n",
    "However, transparency should not be abandoned, but rather **strategically calibrated**: the goal is to identify which information should be made public and which should remain protected or filtered to ensure overall system security. This approach requires close collaboration between security researchers, AI developers, and communication specialists.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Governance and Accountability: Knowing Who Is Responsible</span>\n",
    "\n",
    "Transparency also requires clear governance. It must be possible to identify the developers or organizations responsible for an AI system. This traceability is essential, especially in case of disputes. It empowers individuals involved to know who is responsible for a system’s decisions and for any potential errors.\n",
    "\n",
    "Systems should maintain a **log** of decisions made, enabling future **[audits](../Source/keywords.ipynb#audits)**. These audits can be conducted by authorities or independent experts ([Source: arXiv – Post-market Monitoring](https://arxiv.org/abs/2111.05071)).\n",
    "\n",
    "Moreover, users should have access to **redress mechanisms**, allowing them to contest automated decisions and request human review a right recognized under the [GDPR](https://www.cnil.fr/fr/reglement-europeen-protection-donnees). A good AI system should include **[post-deployment monitoring](../Source/keywords.ipynb#post-deployment-monitoring)** to adapt to evolving contexts or detect potential drifts.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee8ccd-34ed-4835-ad6c-38ff1f58b82c",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Clarity for the Public: Readability and Accessibility</span>\n",
    "\n",
    "It is essential that information related to an AI system is understandable to the general public. This requires the use of simple vocabulary tailored to end users.\n",
    "\n",
    "The quantity and nature of the information provided should also be adapted to the user's role: a regular user does not need the same level of detail as an expert or a regulatory authority. This concept is known as contextual transparency.\n",
    "\n",
    "Finally, technical documents, explanatory interfaces, and APIs must be easily accessible. This fosters trust and promotes the democratization of AI usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991c88b-719c-473b-9aa4-76d24ce27311",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Regulatory Compliance: AI Act and International Standards</span>\n",
    "\n",
    "Compliance with legal frameworks is another pillar of transparency. The **[AI Act](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)** classifies AI systems by risk level in **Europe** and imposes specific requirements based on that classification.\n",
    "\n",
    "Beyond European legislation, it is also important to adhere to international standards such as those set by the [OECD](https://www.oecd.org/en/ai/principles/), UNESCO, or ISO standards. This ensures alignment with global regulations and enables broader deployment of AI systems without having to retrofit them later to meet standards that were not considered during the system’s initial design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d604a-30b1-4bcf-be95-ac3e84709d66",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Future Perspectives</span>\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Toward “Useful” Transparency</span>\n",
    "\n",
    "The evolution toward **<span style=\"color:#228B22;\">useful</span>** transparency is based on the concept of **<span style=\"color:#228B22;\">graduated</span>** transparency, which adapts the information provided according to the **<span style=\"color:#228B22;\">needs of different stakeholders</span>**. This involves considering all stakeholders and understanding both their needs and their level of knowledge on the subject.  \n",
    "For example, **<span style=\"color:#228B22;\">end users</span>** require simple explanations focused on the system’s impact, while **<span style=\"color:#228B22;\">technical experts</span>** need more detailed and concise information to evaluate system compliance or robustness.  \n",
    "This graduated transparency would make it possible to **prioritize clarity and relevance** over sheer quantity of information, reducing the risk of interpretive opacity and thus user misunderstanding.  \n",
    "The ATVAIS report emphasizes this need for “contextual intelligence” in transparency to **<span style=\"color:#228B22;\">balance ethical, regulatory, and operational requirements</span>** ([ATVAIS, 2023](https://atvais.com/transparency-paradox-in-ai-governance/)).\n",
    "\n",
    "However, this model assumes a solid understanding of user profiles, their needs, and their knowledge as well as the constantly evolving norms and use cases. \"Useful\" transparency must not become an excuse for arbitrarily omitting or diluting important information.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">The Role of Data and AI Literacy</span>\n",
    "\n",
    "Transparency alone is not sufficient if the target audiences lack the **skills** to interpret and **<span style=\"color:#228B22;\">use the information</span>** provided.  \n",
    "We must therefore educate those affected by the need for transparency so that they can understand even data that may not be easily simplified.  \n",
    "Data and AI literacy emerges as a **<span style=\"color:#228B22;\">central lever</span>** for strengthening trust, understanding, and critical thinking.  \n",
    "Training users and citizens on the issues, limitations, and methods of AI enables them to more effectively **<span style=\"color:#228B22;\">question systems</span>**, identify **biases**, and actively participate in ethical and regulatory debates surrounding AI usage.  \n",
    "Recent initiatives in several countries (e.g., school programs, MOOCs, citizen workshops) demonstrate the positive impact of this approach on **<span style=\"color:#228B22;\">digital empowerment</span>** ([OECD, 2021](https://www.oecd.org/education/ai-education.htm)).\n",
    "\n",
    "However, education is a long term process with gradual effects and should not exempt AI developers from their responsibility to ensure transparency that is both appropriate and regulated.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">The Importance of Co-Constructing Standards</span>\n",
    "\n",
    "The legitimacy of transparency practices largely depends on their **<span style=\"color:#228B22;\">social acceptance</span>**, which requires the **<span style=\"color:#228B22;\">co-construction</span>** of standards involving technical experts, public institutions, and citizens.  \n",
    "This collaboration helps define **what should be transparent**, how that transparency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbbf26-2566-4e96-8db9-3ec274eb776b",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## <a id=\"conclusion\"></a><span style=\"color:#1E90FF;\">Conclusion</span>  \n",
    "<span style=\"color:#1E90FF;\"><strong>AI transparency</strong></span> is a <strong>complex balancing act</strong> between explaining decisions (through tools like **SHAP, LIME, WIT**), ensuring <span style=\"color:#FF4500;\"><strong>safety</strong></span> and **<span style=\"color:#DC143C;\">data protection</span>**, preserving <span style=\"color:#8B008B;\"><strong>trade secrets</strong></span>, and guaranteeing <span style=\"color:#228B22;\"><strong>smooth accessibility</strong></span> for all audiences, regardless of background.  \n",
    "\n",
    "Technical and regulatory requirements including the identification of responsible parties, **traceability**, **auditability**, **redress mechanisms**, and compliance with the **GDPR**, **AI Act**, and **OECD standards** are essential to building trust in **ML** systems.  \n",
    "\n",
    "However, exposing too much information (such as saliency maps) can introduce **real risks**: model reconstruction, leakage of sensitive data via membership inference, or industrial espionage. Techniques such as **differential privacy**, **homomorphic encryption**, and **watermarking** help reconcile explainability with confidentiality.  \n",
    "\n",
    "A **strategically graduated transparency**, combined with co-construction of standards and citizen education, is therefore essential. This is a **dynamic process**, to be developed collaboratively with researchers, regulators, organizations, and citizens, to ensure that AI remains **reliable, fair, accountable**, and above all, **in service of humanity**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51edeea5-8f2b-45c7-bd3e-e5c81d9c4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run ./Fonctions/01-Scalabilite_transparence.ipynb\n",
    "# %run ./Fonctions/03-graphique.ipynb\n",
    "# %run ./Fonctions/04-table_bdd.ipynb\n",
    "# %run ./Fonctions/05-Ajouter_relation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3089e68-da6f-4c63-9b1e-3568ba9a373d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
