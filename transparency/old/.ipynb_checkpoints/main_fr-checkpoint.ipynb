{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45a4ed1-6bb9-426f-a15f-f71504cd602c",
   "metadata": {},
   "source": [
    "# <span style=\"color:#1E90FF;\"> The concept of transparency on AI </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb767a0-3e97-4f53-ba33-69cc1485c217",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">INTRODUCTION</span>  \n",
    "La transparence en IA est actuellement un sujet à débat concernant l'**éthique** et **la réglementation**. Cette transparence vise à renforcer la <span style=\"color:#DC143C;\">**confiance**</span>, la <span style=\"color:#DC143C;\">**responsabilité**</span> et l'<span style=\"color:#DC143C;\">**acceptabilité**</span> des systèmes d’intelligence artificielle ([source Springer](https://link.springer.com/article/10.1007/s00146-020-00960-w)).Cependant il est compliqué de mettre en place une transparence complète au vu de la <span style=\"color:#FFA500;\">**disponibilité d’information**</span>, de sa <span style=\"color:#FFA500;\">**lisibilité**</span> et des informations corrélé  à la <span style=\"color:#FF0000;\">**vie privée**</span>. Nous allons voir comment faire face à ces paradoxes en concevant une transparence à la fois <span style=\"color:#228B22;\">**pertinente**</span> et <span style=\"color:#228B22;\">**compréhensible**</span> pour toute personnes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a136c35-4f61-4588-87fe-66e8232e4f84",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Comprendre la transparence en IA</span>\n",
    "\n",
    "La transparence en intelligence artificielle (IA) consiste à rendre visibles, compréhensibles et explicables les décisions prises par ce système. C’est un gros enjeu aujourd'hui avec toutes les décisions prises automatiquement comme l’analyse d'un CV, le tri de données,.... Ces décisions prises par l'ia sont directement influencés/dirigé par des modèles de machine learning ([ML](../Src/Mots-clés.ipynb#ML)\n",
    "). Dans ce contexte, la transparence devient essentielle pour garantir l’**équité**, la **fiabilité**, et la **responsabilité**.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Est-ce que tout le monde comprends la même transparence ?</span>\n",
    "\n",
    "Rendre un système d’IA “transparent” ne garantit pas que son fonctionnement sera **<span style=\"color:#228B22;\">réellement compris</span>**. La **<span style=\"color:#FF8C00;\">visualisation de données</span>** ou l’**<span style=\"color:#FF8C00;\">affichage de paramètres</span>** techniques peuvent au contraire conduire à davantage d’**<span style=\"color:#FF4500;\">opacité cognitive</span>** si les informations ne sont pas vulgarisées pour les personnes concernés. On passe ainsi d’une opacité algorithmique à une **<span style=\"color:#FF4500;\">incompréhension interprétative</span>** : ce n’est plus la boîte noire du système qui pose problème, mais la complexité du langage utilisé pour la décrire.\n",
    "\n",
    "Ce phénomène est mis en évidence dans une étude menée par Stanford ([Foundation Model Transparency Index de 2023](https://crfm.stanford.edu/fmti/October-2023/index.html)), qui explore les mécanismes de **<span style=\"color:#228B22;\">confiance dans les systèmes algorithmiques</span>**. L’étude montre que fournir trop de détails techniques sur les paramètres ou les justifications internes peut **<span style=\"color:#FF0000;\">réduire la confiance</span>** des utilisateurs au lieu de la renforcer. Cela souligne l’importance cruciale de produire une **<span style=\"color:#228B22;\">explication intelligible</span>**, non pas pour les connaisseurs, mais pour les utilisateurs finaux, souvent non initiés ([étude sur PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC7755865/)).\n",
    "\n",
    "Dans cette continuité, la transparence n'est pas une **quantité d’information à livrer**,il s'agit plutôt d'un **travail de traduction pédagogique**. \n",
    "Ce qui implique de:  \n",
    "- choisir un niveau de langage adapté,\n",
    "- expliquer les choix algorithmiques à l’aide de métaphores, d’illustrations ou de comparaisons concrètes,  \n",
    "- anticiper les besoins de vulgarisation d'un sujet technique des différents publics (citoyens, professionnels, décideurs).\n",
    "\n",
    "Ce principe rejoint celui de “transparence ciblée” ou “explanation-by-design” développé dans le champ de l’**XAI (eXplainable AI)** : une IA explicable n’est pas seulement lisible par la machine ou le développeur, mais doit être **utile, accessible et interprétable par l'humain** qui l’utilise. Le défi n’est donc pas uniquement de montrer, mais de faire **comprendre** à l'utilisateur ce qu'il utilise et comment cela fonctionne au vu des avancées technologiques.\n",
    "\n",
    "[Voir l’article complet PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC7268993/)\n",
    "\n",
    "### <span style=\"color:#8B008B;\"> Entre transparence et confidentialité  </span>\n",
    "Le \"**paradoxe de la transparence** désigne la difficulté à trouver un équilibre : trop peu d’informations nuit à la <span style=\"color:#DC143C;\">**confiance**</span> mais trop d’informations provoque de la <span style=\"color:#FF8C00;\">**confusion**</span> ce qui entraine une perte de <span style=\"color:#DC143C;\">**légitimité**</span>. Une transparence totale peut donc être <span style=\"color:#FF0000;\">**contre-productive**</span>, voire <span style=\"color:#FF0000;\">**dangereuse**</span>. Cela se voit aussi sur les sujets confidentiels comme les informations privées des utilisateurs, pour une bonne transparence il faudrait avoir des **logs** ainsi que des explications sur ces données. Cependant quand cela touche à la vie privée des utilisateurs, la question de la légitimité de la demande d'information se pose.\n",
    "\n",
    "Ce constat met en lumière plusieurs dimensions essentielles du **paradoxe de transparence** :\n",
    "\n",
    "- Une transparence **<span style=\"color:#DC143C;\">insuffisante</span>** peut miner la **<span style=\"color:#DC143C;\">confiance</span>** des utilisateurs et alimenter la méfiance vis-à-vis des systèmes d’IA.  \n",
    "- À l’inverse, une transparence **<span style=\"color:#FF8C00;\">trop exhaustive</span>** peut entraîner une **<span style=\"color:#FF8C00;\">confusion</span>** et, paradoxalement, affaiblir la **<span style=\"color:#DC143C;\">légitimité perçue</span>** des systèmes automatisés.  \n",
    "- Ce phénomène, qualifié de **<span style=\"color:#FF0000;\">transparency paradox</span>**, désigne le fait qu’un excès d’informations mal hiérarchisées ou trop techniques peut rendre la compréhension plus difficile, voire **<span style=\"color:#FF0000;\">contre-productive</span>** sur le plan opérationnel.\n",
    "\n",
    "[Voir l’article complet sur Springer](https://link.springer.com/article/10.1007/s00146-020-00960-w)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccff32-a3b6-462c-b9ca-20fa731c3c6e",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">L’explicabilité : comprendre les décisions de l’IA </span>\n",
    "\n",
    "Un aspect fondamental de la transparence est l’explicabilité des décisions. Elle se divise en deux volets :\n",
    "\n",
    "- **Explications locales**, qui permettent de comprendre pourquoi une décision spécifique a été prise. Des outils comme **SHAP** (Shapley Additive Explanations) et **LIME** (Local Interpretable Model-Agnostic Explanations) sont aujourd’hui des références. Ils aident à expliquer comment un modèle est arrivé à une prédiction individuelle, par exemple pourquoi un prêt a été refusé à une personne. \n",
    "\n",
    "- **Explications globales**, qui permettent de cerner le fonctionnement général d’un modèle. Cela est particulièrement utile pour les régulateurs ou les chercheurs qui évaluent les risques globaux d’un système.\n",
    "\n",
    "Des outils comme **WIT** (What-If Tool) permettent de tester visuellement les effets de modifications d'entrée sur les sorties du modèle, améliorant ainsi cette transparence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c295a6-de6c-420c-af1d-72fcdc78542a",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">RISQUES</span>  \n",
    "\n",
    "### <span style=\"color:#8B008B;\">Exposition des secrets industriels</span>\n",
    "\n",
    "La divulgation des algorithmes et des mécanismes internes des systèmes d’IA, bien que nécessaire à la transparence, présente un risque important pour la **<span style=\"color:#8B008B;\">propriété intellectuelle</span>** et la compétitivité des entreprises. Pour l'exemple, prenons les plans d'architecte d'un nouveau centre commercial. Si en questionnant correctement l'IA nous pouvions recréér partiellement voir complètement le travail d'une entreprise privée alors les secrets professionnels de conception et le temps de travail des employés seraient comme **volés** par la personne à l'origine de cette recherche\n",
    "  \n",
    "Un article de *Forbes* montre et demande de faire attention au “**paradoxe de la transparence**” : en voulant tout révéler, les entreprises s’exposent à des risques de **perte d’avantage concurrentiel** et à des stratégies d’**extraction malveillante** ([Forbes, 2021](https://www.forbes.com/sites/aparnadhinakaran/2021/09/10/overcoming-ais-transparency-paradox/)).\n",
    "\n",
    "Cependant ce risuqe est nuancable: certaines méthodes de protection comme le **chiffrement homomorphe**, les **techniques de watermarking** ou la **fédération des données** permettent aujourd’hui de préserver la confidentialité tout en améliorant l’explicabilité. La recherche sur la cohabitation entre transparence et protection industrielle est donc en pleine évolution.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Fuite de données personnelles</span>\n",
    "\n",
    "L'explicabilité exposent aussi un risque sérieux pour la **<span style=\"color:#8B008B;\">confidentialité des données</span>**.  \n",
    "Des attaques dites de **membership inference** peuvent, à partir d’une explicationt, déterminer si une donnée individuelle faisait partie de l’ensemble d’entraînement, révélant ainsi des informations privées. De même, les **prompt leaks** dans les modèles génératifs peuvent involontairement divulguer des fragments de données sensibles présentes dans les corpus d’entraînement.  \n",
    "Une étude publiée sur *arXiv* détaille ces vulnérabilités, montrant que des informations personnelles telles que des noms, adresses ou antécédents médicaux peuvent être compromises via les explications produites ([arXiv, 2019](https://arxiv.org/abs/1907.00164)).\n",
    "\n",
    "Il est important de reconnaître que ces risques dépendent des **méthodes d’explicabilité** utilisées par les communiquants et des **protocoles de protection** mis en place. Par exemple, des techniques comme la **differential privacy** peuvent atténuer ces fuites.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Tension entre transparence et sécurité</span>\n",
    "\n",
    "La **<span style=\"color:#FF4500;\">tension entre transparence et sécurité</span>** illustre un dilemme , souvent critiqué mais pas sans fondements pratiques.  \n",
    "Une divulgation trop large et mal maîtrisée des détails techniques peut fournir aux attaquants des informations leur permettant de contourner ou compromettre un système, en exploitant ses **failles intrinsèques**.  \n",
    "Une publication sur *arXiv* montre que la transparence complète (sans filtre de confidentialité) peut augmenter la surface d’attaque, diminuant par définition la robustesse des systèmes d’IA ([arXiv, 2025](https://arxiv.org/abs/2501.18669)).\n",
    "\n",
    "Cependant la transparence ne doit pas être abandonnée, mais elle doit être **stratégiquement calibrée** : il s’agit d’identifier quelles informations doivent être rendues publiques, et quelles autres doivent rester protégées ou filtrées pour garantir la sécurité globale. Cette approche impose une collaboration étroite entre chercheurs en sécurité, développeurs d’IA, et commniquant.\n",
    "\n",
    "### <span style=\"color:#8B008B;\"> Gouvernance et redevabilité : savoir qui est responsable</span>\n",
    "\n",
    "La transparence implique aussi une gouvernance claire. Il doit être possible d’identifier les développeurs ou organisations responsables d’un système IA. Cette traçabilité est essentielle, notamment en cas de litige. Elle permet de donner la connaissance aux personnes impliqués de la personne responsable des décisions et des possibles fautes des décisions du système.\n",
    "\n",
    "Les systèmes doivent conserver un **historique** (logs) des décisions prises, pour permettre des audits ultérieurs. Ces **audits** peuvent être réalisés par des autorités ou des experts indépendants ([Source arXiv – Post-market Monitoring](https://arxiv.org/abs/2111.05071)).\n",
    "\n",
    "De plus, les utilisateurs doivent bénéficier de **mécanismes de recours**, leur permettant de contester une décision automatisée et de demander une révision humaine, un droit reconnu par le [RGPD](https://www.cnil.fr/fr/reglement-europeen-protection-donnees). Un bon système IA se doit d'avoir un **suivi post-déploiement** pour s’adapter à l’évolution du contexte ou détecter d’éventuelles dérives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5b5a9-bfe6-418d-ba28-4ce92680a5a5",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Clarté pour le public : lisibilité et accessibilité</span>\n",
    "\n",
    "Il est impératif que les informations relatives à une IA soient compréhensibles par le public. Cela implique un vocabulaire simple, adapté aux utilisateurs finaux. \n",
    "\n",
    "Il faut également adapter la quantité et la nature des informations fournies selon le rôle : un usager n’aura pas besoin du même niveau de détail qu’un expert ou une autorité de régulation. Ce concept est appelé transparence contextuelle.\n",
    "\n",
    "Enfin, les documents techniques, interfaces explicatives et API doivent être facilement accessibles. Cela favorise la confiance et la démocratisation de l’usage de l’IA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f213af-3c3f-4edf-8171-e1aacfc99a2c",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Conformité réglementaire : IA Act et normes internationales </span>\n",
    "\n",
    "La conformité à des cadres juridiques est un autre pilier de la transparence. Le **[AI Act](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)** classe les IA selon leur niveau de risque en **Europe** et impose des exigences spécifiques selon ce classement.\n",
    "\n",
    "Au-delà des lois européenne, il est important de suivre des normes internationales telles que celles définies par l’[OCDE](https://www.oecd.org/fr/themes/principes-de-l-ia.html), l’UNESCO ou les normes ISO. Cela permet d'être à jour sur les normes et lois de tous le monde permettant ainsi d'étendre l'influence de son IA sans devoir reprendre des parties de celle ci pour coller à des normes non pris en compte à la construction de la structure du système informatique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51acc1-4d30-4fc1-a275-b6583a648199",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Perspectives d’évolution</span>\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Vers une transparence “utile”</span>\n",
    "\n",
    "L’évolution vers une transparence **<span style=\"color:#228B22;\">utile</span>** repose sur le concept d’une transparence **<span style=\"color:#228B22;\">graduée</span>**, concept qui adapte les informations à disposition en fonction des **<span style=\"color:#228B22;\">besoins des différentes parties prenantes</span>**. Cela implique de faire attention à toutes les parties prenantes et de connaître leurs besoins ainsi que leurs niveau de connaissance sur le sujet.\n",
    "Ainsi, les **<span style=\"color:#228B22;\">utilisateurs finaux</span>** requièrent des explications simples et centrées sur les impacts du système expliqué, tandis que les **<span style=\"color:#228B22;\">experts techniques</span>** ont besoin d’informations plus détaillées et concises pour évaluer la conformité ou la robustesse des systèmes.  \n",
    "Cette transparence graduée permettrais de **prioriser la clarté et la pertinence** des informations plutôt que leur simple existance, réduisant ainsi le risque d’opacité interprétative et donc d'incompréhension de la personne visée.  \n",
    "Le rapport d’ATVAIS souligne cette nécessité d’“intelligence contextuelle” dans la transparence pour **<span style=\"color:#228B22;\">équilibrer exigences éthiques, réglementaires et opérationnelles</span>** ([ATVAIS, 2023](https://atvais.com/transparency-paradox-in-ai-governance/)).\n",
    "\n",
    "Cependant, ce modèle suppose une bonne connaissance des profils utilisateurs, de leurs besoins et connaissance. Ainsi que des usages et normes toujours en évolution. Ce qui reste un défi dans des contextes complexes et évolutifs. La transparence “utile” ne doit pas devenir un prétexte à la dilution ou à la sélection arbitraire d’informations.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Rôle de l’éducation aux données et à l’IA</span>\n",
    "\n",
    "La transparence ne suffit pas si les publics ciblés ne disposent pas des **compétences** pour interpréter et **<span style=\"color:#228B22;\">utiliser les informations</span>** données.  \n",
    "Nous devons donc éduquer les personnes ciblées par ce besoin de transparence pour qu'ils puissent comprendre les données parfois non vulgarisable sur leurs questions.\n",
    "L’éducation aux données et à l’intelligence artificielle apparaît donc comme un **<span style=\"color:#228B22;\">levier central</span>** pour renforcer la confiance, la compréhension et la capacité critique.  \n",
    "Former les utilisateurs et citoyens aux enjeux, aux limites ainsi qu'aux méthodes de l’IA permet de mieux remettre **<span style=\"color:#228B22;\">question les systèmes</span>**, de repérer les **biais**, et de participer activement aux débats éthiques et réglementaires que peuvent soulever l'utilisation d'une IA.  \n",
    "Des initiatives récentes dans plusieurs pays (ex. : programmes scolaires, MOOCs, ateliers citoyens) montrent l’impact positif de cette démarche sur l’**<span style=\"color:#228B22;\">empowerment numérique</span>** ([OECD, 2021](https://www.oecd.org/education/ai-education.htm)).\n",
    "\n",
    "Toutefois, l’éducation est un processus long, aux effets graduels, et ne doit pas dispenser les développeurs d’IA d’assumer leur responsabilité en matière de transparence adaptée et régulée.\n",
    "\n",
    "### <span style=\"color:#8B008B;\">Importance de la co-construction des normes</span>\n",
    "\n",
    "La légitimité des pratiques de transparence dépend largement de leur **<span style=\"color:#228B22;\">acceptation sociale</span>**, qui passe par la **<span style=\"color:#228B22;\">co-construction</span>** de ces normes associant experts techniques, institutions publiques et citoyens.  \n",
    "Cette collaboration permet de définir  **ce qui doit être transparent**, comment cette transparence doit s’organiser et **pourquoi** certaines informations peuvent et doivent rester protégées (notamment en fonction de la légitimé de l'information et son niveau de confidentialité).  \n",
    "Le domaine de la cybersécurité offre un exemple parkant, où les normes ont été élaborés grâce à des débats et dialogues publiques entre acteurs publics, privés et communautés techniques. Ce qui renforce ainsi leur efficacité et leur crédibilité ([ENISA, 2020](https://www.enisa.europa.eu/topics/csirt-cert-services/csirt-network)).\n",
    "\n",
    "Cette approche de co-construction permet de démocratisé l'utilisation et les normes de l’IA, mais elle doit aussi composer avec les divergences d’intérêts et la complexité croissante des technologies. Le défi est d’instaurer un cadre normatif **<span style=\"color:#228B22;\">à la fois souple, robuste et évolutif</span>**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c34bce-5733-449a-958d-b170c85076b8",
   "metadata": {},
   "source": [
    "## <span style=\"color:#1E90FF;\">Conclusion</span> \n",
    "La <span style=\"color:#1E90FF;\"><strong>transparence en IA</strong></span> est un <strong>équilibre complexe</strong> entre expliquer les décisions (via **SHAP, LIME, WIT**), respecter la <span style=\"color:#FF4500;\"><strong>sûreté</strong></span> et la **<span style=\"color:#DC143C;\">protection des données</span>**, préserver les <span style=\"color:#8B008B;\"><strong>secrets industriels</strong></span>, et garantir une <span style=\"color:#228B22;\"><strong>accessibilité fluide</strong></span> pour tous les publics quels qu'ils soient. Les critères techniques et réglementaires, identification des responsables, **traçabilité**, **auditabilité**, **mécanismes de recours**, et conformité au **RGPD**, **AI Act** et **normes OCDE** sont essentiels pour fonder la confiance dans les systèmes **ML**. Néanmoins, exposer trop d’informations (comme les saliency maps) peut provoquer des **risques réels** : reconstruction de modèles, extraction de données sensibles via membership inference, ou espionnage industriel. Des méthodes telles que la **differential privacy**, l’**homomorphic encryption** et le **watermarking** permettent de concilier explicabilité et confidentialité. Une transparence **stratégiquement graduée**, combinée à une co-construction et une éducation des citoyens, renforcée aux enjeux, est donc indispensable. Il s’agit d’un **processus dynamique**, à construire en équipe avec chercheurs, régulateurs, organisations et citoyens, afin de garantir que l’IA reste **fiable, équitable, responsable**, et surtout **au service de l’humain**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda6a87e-3005-427d-8cf5-60bb3e93a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run ./Fonctions/01-Scalabilite_transparence.ipynb\n",
    "# %run ./Fonctions/03-graphique.ipynb\n",
    "%run ./Fonctions/04-table_bdd.ipynb\n",
    "#%run ./Fonctions/05-Ajouter_relation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82daa91d-1057-453c-ba97-eeca97f64394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a71506-946e-49e8-917a-f3f1cb852ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
